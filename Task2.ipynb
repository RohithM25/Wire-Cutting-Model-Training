{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regression import add_neighborhood_feature, avg_loss, SGD, sigmoid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #code to generate data\n",
    "# def makeTask1Sets(N, dest):\n",
    "#     matrices = []\n",
    "#     labels = []\n",
    "#     for _ in range(int(N)):\n",
    "#         dgrm = diagram.createDiagram()\n",
    "#         danger = np.array(dgrm.layWires())\n",
    "#         matrices.append(dgrm.data.flatten())\n",
    "#         labels.append(danger)\n",
    "#     matrices = np.array(matrices)\n",
    "#     labels = np.array(labels)\n",
    "\n",
    "#     # In case the necessary directory hasn't been made yet\n",
    "#     #os.makedirs(dest, exist_ok=True)\n",
    "#     np.save(arr=matrices,file=f'{dest}/data.npy',allow_pickle=True)\n",
    "#     np.save(arr=labels,file=f'{dest}/labels.npy',allow_pickle=True)\n",
    "\n",
    "# def makeTask2Sets(N, dest):\n",
    "#     matrices = []\n",
    "#     labels = []\n",
    "\n",
    "#     count=0\n",
    "#     while(count<int(N)):\n",
    "#         dgrm = diagram.createDiagram()\n",
    "#         wireToCut = np.array((dgrm.layWires())[1])\n",
    "#         #print(f\"my wire to cut: {wireToCut}\")\n",
    "#         if not np.array_equal(wireToCut,(0,0,0,0,1)):\n",
    "#             matrices.append(dgrm.data.flatten())\n",
    "#             labels.append(wireToCut)\n",
    "#             count+=1\n",
    "\n",
    "#     matrices = np.array(matrices)\n",
    "#     labels = np.array(labels)\n",
    "\n",
    "#     # In case the necessary directory hasn't been made yet\n",
    "#     #os.makedirs(dest, exist_ok=True)\n",
    "#     np.save(arr=matrices,file=f'{dest}/data.npy',allow_pickle=True)\n",
    "#     np.save(arr=labels,file=f'{dest}/labels.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load in data: \n",
    "    Input Space: (1200, 1) one hot encoded data \"\"\"\n",
    "\n",
    "x = np.load('OHE_Task2_Trainset2000\\data.npy')\n",
    "y = np.load('OHE_Task2_Trainset2000\\labels.npy')\n",
    "w1 = np.random.rand(2000)*2 - np.ones(shape=2000)\n",
    "w2 = np.random.rand(2000)*2 - np.ones(shape=2000)\n",
    "w3 = np.random.rand(2000)*2 - np.ones(shape=2000)\n",
    "w4 = np.random.rand(2000)*2 - np.ones(shape=2000)\n",
    "w5 = np.random.rand(2000)*2 - np.ones(shape=2000)\n",
    "weights = np.vstack((w1,w2,w3,w4,w5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2000,5) (5,2000) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 47\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinalLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)\n\u001b[1;32m---> 47\u001b[0m feed_forward_multiclass(x, y, num_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[137], line 41\u001b[0m, in \u001b[0;36mfeed_forward_multiclass\u001b[1;34m(data, labels, num_iters)\u001b[0m\n\u001b[0;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     40\u001b[0m y \u001b[38;5;241m=\u001b[39m labels\n\u001b[1;32m---> 41\u001b[0m loss \u001b[38;5;241m=\u001b[39m SGD2(weights, x, y, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)\n",
      "Cell \u001b[1;32mIn[137], line 22\u001b[0m, in \u001b[0;36mSGD2\u001b[1;34m(weights, x, ohe_y, alpha)\u001b[0m\n\u001b[0;32m     20\u001b[0m     logits\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mdot(w, x))\n\u001b[0;32m     21\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m softmax(logits)\n\u001b[1;32m---> 22\u001b[0m cur_loss \u001b[38;5;241m=\u001b[39m cross_entropy_loss(ohe_y, y_pred)\n\u001b[0;32m     23\u001b[0m train \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x\u001b[38;5;241m.\u001b[39mT[:,np\u001b[38;5;241m.\u001b[39mnewaxis] , (y_pred \u001b[38;5;241m-\u001b[39m ohe_y))\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mshape(train))\n",
      "Cell \u001b[1;32mIn[137], line 14\u001b[0m, in \u001b[0;36mcross_entropy_loss\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     12\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-15\u001b[39m\n\u001b[0;32m     13\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(y_pred, epsilon, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m epsilon)\n\u001b[1;32m---> 14\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(y_true \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(y_pred)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_true)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2000,5) (5,2000) "
     ]
    }
   ],
   "source": [
    "\"\"\"Multi-Class Classification Model \n",
    "    SoftMax --> computes probabilites for each class totals to 1\n",
    "    Loss Function --> categorical cross entropy --> for all classes -log(softmax(y'))\n",
    "    Deriviate of Loss --> y' - y_actual\"\"\"\n",
    "\n",
    "def softmax(output_x=list): # output x takes 4 diff values for each weight layer\n",
    "    exp_output = np.exp(output_x - np.max(output_x))\n",
    "    probs = exp_output / np.sum(exp_output, axis=-1, keepdims=True)\n",
    "    return probs\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    loss = -np.sum(y_true * np.log(y_pred)) / len(y_true)\n",
    "    return loss\n",
    "\n",
    "def SGD2(weights, x, ohe_y, alpha):\n",
    "    logits = []\n",
    "    for w in weights:\n",
    "        logits.append(np.dot(w, x))\n",
    "    y_pred = softmax(logits)\n",
    "    cur_loss = cross_entropy_loss(ohe_y, y_pred)\n",
    "    train = alpha * np.dot(x.T[:,np.newaxis] , (y_pred - ohe_y))\n",
    "    print(\"trainshape\", np.shape(train))\n",
    "    print(\"TrainValues\", train)\n",
    "    weights -= train\n",
    "    logits.clear()\n",
    "    return cur_loss\n",
    "\n",
    "\n",
    "def feed_forward_multiclass(data, labels, num_iters):\n",
    "    w1 = np.random.rand(2000)*2 - np.ones(shape=2000)\n",
    "    w2 = np.random.rand(2000)*2 - np.ones(shape=2000)\n",
    "    w3 = np.random.rand(2000)*2 - np.ones(shape=2000)\n",
    "    w4 = np.random.rand(2000)*2 - np.ones(shape=2000)\n",
    "    w5 = np.random.rand(2000)*2 - np.ones(shape=2000)\n",
    "    weights = np.vstack((w1,w2,w3,w4,w5))\n",
    "    for i in range(num_iters):\n",
    "        x = data\n",
    "        y = labels\n",
    "        loss = SGD2(weights, x, y, alpha=0.1)\n",
    "        if i == 1:\n",
    "            print(\"InitLoss\", loss)\n",
    "    print(\"FinalLoss\", loss)\n",
    "            \n",
    "\n",
    "feed_forward_multiclass(x, y, num_iters=10)\n",
    "# logits = []\n",
    "# for w in weights:\n",
    "#     logits.append(np.dot(x, w))\n",
    "# print(\"logits\", logits)\n",
    "\n",
    "# print(\"softmax logits\", softmax(logits))\n",
    "# print(\"softmax sum\", np.sum(softmax(logits)))\n",
    "# print(\"cur loss\", cross_entropy_loss(y, softmax(logits)))\n",
    "# train = 1 * (softmax(logits) - y)\n",
    "# print(\"trainshape\", np.shape(train))\n",
    "# print(\"training iteration value\", (weights[0][0]))\n",
    "# weights -= train.T\n",
    "# # for index in range(len(weights)):\n",
    "# #         cur_train = train[index]\n",
    "# #         print(np.shape(cur_train))\n",
    "# #         cur_weight = weights[index]\n",
    "# #         cur_weight -= cur_train\n",
    "# #         for w in cur_weight:\n",
    "# #             print(np.shape(w))\n",
    "# #             w -= cur_train\n",
    "# print(\"training iteration value\", (weights[0][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
