{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regression import add_neighborhood_feature, avg_loss, SGD, sigmoid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load in data: \n",
    "    Input Space: (1200, 1) one hot encoded data \"\"\"\n",
    "\n",
    "x = np.random.rand(1200)*2 - np.ones(shape=1200)\n",
    "# w1 = np.random.rand(1200)*2 - np.ones(shape=1200)\n",
    "# w2 = np.random.rand(1200)*2 - np.ones(shape=1200)\n",
    "# w3 = np.random.rand(1200)*2 - np.ones(shape=1200)\n",
    "# w4 = np.random.rand(1200)*2 - np.ones(shape=1200)\n",
    "# weights = np.vstack((w1,w2,w3,w4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2528110994409043\n",
      "3.2528110994409043\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Multi-Class Classification Model \n",
    "    SoftMax --> computes probabilites for each class totals to 1\n",
    "    Loss Function --> categorical cross entropy --> for all classes -log(softmax(y'))\n",
    "    Deriviate of Loss --> y' - y_actual\"\"\"\n",
    "\n",
    "def softmax(output_x=list): # output x takes 4 diff values for each weight layer\n",
    "    exp_output = np.exp(output_x - np.max(output_x))\n",
    "    probs = exp_output / np.sum(exp_output, axis=-1, keepdims=True)\n",
    "    return probs\n",
    "\n",
    "# logits = []\n",
    "# for w in weights:\n",
    "#     logits.append(np.dot(w, x))\n",
    "# probs = softmax(logits)\n",
    "# print(probs)\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15  # Small constant to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip predicted values to avoid log(0) or log(1)\n",
    "    loss = -np.sum(y_true * np.log(y_pred)) / len(y_true)\n",
    "    return loss\n",
    "\n",
    "actual_probs = [0.15,0.2,0.55,0.1]\n",
    "# loss = cross_entropy_loss(actual_probs, probs)\n",
    "\n",
    "def SGD2(weights, x, ohe_y, alpha):\n",
    "    logits = []\n",
    "    for w in weights:\n",
    "        logits.append(np.dot(w, x))\n",
    "    y_pred = softmax(logits)\n",
    "    cur_loss = cross_entropy_loss(ohe_y, y_pred)\n",
    "    # print(cur_loss)\n",
    "    train = alpha * (y_pred - ohe_y)\n",
    "    # print(train)\n",
    "    for index in range(len(weights)):\n",
    "        cur_train = train[index]\n",
    "        cur_weight = weights[index]\n",
    "        for w in cur_weight:\n",
    "            w -= cur_train\n",
    "    logits.clear()\n",
    "    return cur_loss\n",
    "\n",
    "\n",
    "def feed_forward_multiclass(data, labels, num_iters):\n",
    "    w1 = np.random.rand(1200)*2 - np.ones(shape=1200)\n",
    "    w2 = np.random.rand(1200)*2 - np.ones(shape=1200)\n",
    "    w3 = np.random.rand(1200)*2 - np.ones(shape=1200)\n",
    "    w4 = np.random.rand(1200)*2 - np.ones(shape=1200)\n",
    "    weights = np.vstack((w1,w2,w3,w4))\n",
    "    for i in range(num_iters):\n",
    "        x = data\n",
    "        y = labels\n",
    "        loss = SGD2(weights, x, y, alpha=1)\n",
    "        if i == 1 or i == 4500:\n",
    "            print(loss)\n",
    "\n",
    "feed_forward_multiclass(x, actual_probs, num_iters=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
